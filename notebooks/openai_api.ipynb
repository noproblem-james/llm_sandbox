{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04e74489-d0bb-4c9d-97e7-4e6e945f10c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.16\n"
     ]
    }
   ],
   "source": [
    "! python -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf910b2a-7752-428d-8c3c-97ba48466fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29d44c69-db64-411d-b495-70a73ebf42e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "965dc476-d9da-4d08-8d03-34a9d64eecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11727a19-f972-4721-8ae5-8f0afaca6cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 1200\n",
    "TEMP = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76c6ebd-b95d-4cc7-8fb3-9c3a9d96e0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_MODEL = 'gpt-3.5-turbo'\n",
    "TEXT_MODEL = 'text-davinci-003'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "283da5db-295b-4425-8f76-68f4f333429d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Is an XGBoost Classifier a good model to use if you are interested in probability outputs? Reason through it step by step.'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43b149c4-9271-4ce6-b659-df963de1cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c71b10-426c-4c56-a91b-c10125e8a4cc",
   "metadata": {},
   "source": [
    "# Interacting with GPT via API and Python `requests` library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4dbf87-c0f6-4070-80e1-39730030a9d1",
   "metadata": {},
   "source": [
    "## API: Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e55e294-9f7e-4aa9-b49b-01a5efad1915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "API_ENDPOINT = \"https://api.openai.com/v1/chat/completions\"\n",
    "\n",
    "def generate_chat_completion(messages, model, temperature, max_tokens=None, ):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\",\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "    }\n",
    "    if max_tokens is not None:\n",
    "        data[\"max_tokens\"] = max_tokens\n",
    "    response = requests.post(API_ENDPOINT, headers=headers, data=json.dumps(data))\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        raise Exception(f\"Error {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4158fec-5e48-4e61-befc-1ea1327e773f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, an XGBoost Classifier is a good model to use if you are interested in probability outputs. Here's the step-by-step reasoning:\n",
      "\n",
      "1. XGBoost is a popular and powerful machine learning algorithm that is known for its high performance and accuracy in various tasks, including classification problems.\n",
      "\n",
      "2. XGBoost is an ensemble method that combines multiple weak learners (decision trees) to create a strong learner. It uses a gradient boosting framework, which means it iteratively improves the model by minimizing a loss function.\n",
      "\n",
      "3. XGBoost provides a built-in probability output feature. By default, it outputs the predicted class labels, but you can also obtain the probability estimates for each class.\n",
      "\n",
      "4. The probability estimates from XGBoost are reliable and well-calibrated. This means that the predicted probabilities reflect the true likelihood of an instance belonging to each class. This is particularly useful when you need to make decisions based on the confidence or uncertainty of the predictions.\n",
      "\n",
      "5. XGBoost allows you to control the probability output behavior through parameters such as \"objective\" and \"eval_metric\". You can choose different objectives (e.g., binary logistic, multi-class logistic) and evaluation metrics (e.g., log loss, AUC) to optimize the model's performance and probability estimates.\n",
      "\n",
      "6. XGBoost also supports advanced techniques like regularization, early stopping, and cross-validation, which can further improve the model's accuracy and reliability of probability outputs.\n",
      "\n",
      "Overall, XGBoost Classifier is a reliable and effective model for obtaining probability outputs in classification tasks. Its strong performance, well-calibrated probabilities, and flexibility make it a popular choice among data scientists and machine learning practitioners.\n"
     ]
    }
   ],
   "source": [
    "response_text = \\\n",
    "generate_chat_completion(\n",
    "    messages, \n",
    "    model=CHAT_MODEL, \n",
    "    temperature=TEMP, \n",
    "    max_tokens=MAX_TOKENS\n",
    ")\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a1eac6-baf9-4167-bcea-3f47e17c88b1",
   "metadata": {},
   "source": [
    "# Interacting with GPT via `openai` Python SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72e13b8-e263-400f-b2b9-c45a159e7c9a",
   "metadata": {},
   "source": [
    "## SDK: Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a283285-9f6a-49e3-952d-c19cc1ba0349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2536609b-ad0b-4a03-8f09-d59c9ca6eedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57583f72-d3f1-487d-8409-fb5fdd3a6976",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_completion = openai.ChatCompletion.create(model=CHAT_MODEL, messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "568e14a2-b386-48ed-b503-cfdc0ea92d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "openai.openai_object.OpenAIObject"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chat_completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d236abcb-75c7-4462-a6b6-c289a82c62ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'object', 'created', 'model', 'choices', 'usage'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_completion.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac255cf5-ef45-456b-9ae5-f35fd752abd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, XGBoost Classifier is a good model to use if you are interested in probability outputs. Here's the step-by-step reasoning:\n",
      "\n",
      "1. XGBoost is an optimized implementation of the gradient boosting algorithm, known for its high performance and accuracy in machine learning tasks. It is widely used in various domains such as finance, healthcare, and natural language processing.\n",
      "\n",
      "2. XGBoost is capable of producing probability outputs because it uses a decision tree ensemble approach. Decision trees can naturally output probability estimates by assigning a fraction of training samples to each leaf node. The ensemble of decision trees in XGBoost can provide more accurate and reliable probability estimates compared to individual decision trees.\n",
      "\n",
      "3. XGBoost can handle both binary and multiclass classification tasks. For binary classification, the default output of XGBoost is the probability of the positive class (1) as opposed to the negative class (0). So, you can directly obtain the probability output for the positive class.\n",
      "\n",
      "4. XGBoost provides a flexible interface for controlling the probability outputs. It allows you to specify the objective function and evaluation metric during the model training. You can choose from various objective functions such as binary logistic regression or multiclass softmax, depending on your specific problem.\n",
      "\n",
      "5. XGBoost incorporates regularization techniques to avoid overfitting and improve generalization. Regularization helps in producing well-calibrated probability outputs by preventing extreme or overconfident predictions.\n",
      "\n",
      "6. XGBoost supports hyperparameter tuning, which allows you to optimize the model performance and adjust the balance between accuracy and calibration of probability estimates. This can be done through techniques like cross-validation and grid search.\n",
      "\n",
      "In conclusion, XGBoost Classifier is not only a powerful and accurate model but also provides reliable and well-calibrated probability outputs, making it a suitable choice when probability estimation is of interest.\n"
     ]
    }
   ],
   "source": [
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "059816fb-04b3-4d35-8a47-1652bc497c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, XGBoost Classifier is a good model to use if you are interested in probability outputs. Here's the step-by-step reasoning:\n",
      "\n",
      "1. XGBoost is an optimized implementation of the gradient boosting algorithm, known for its high performance and accuracy in machine learning tasks. It is widely used in various domains such as finance, healthcare, and natural language processing.\n",
      "\n",
      "2. XGBoost is capable of producing probability outputs because it uses a decision tree ensemble approach. Decision trees can naturally output probability estimates by assigning a fraction of training samples to each leaf node. The ensemble of decision trees in XGBoost can provide more accurate and reliable probability estimates compared to individual decision trees.\n",
      "\n",
      "3. XGBoost can handle both binary and multiclass classification tasks. For binary classification, the default output of XGBoost is the probability of the positive class (1) as opposed to the negative class (0). So, you can directly obtain the probability output for the positive class.\n",
      "\n",
      "4. XGBoost provides a flexible interface for controlling the probability outputs. It allows you to specify the objective function and evaluation metric during the model training. You can choose from various objective functions such as binary logistic regression or multiclass softmax, depending on your specific problem.\n",
      "\n",
      "5. XGBoost incorporates regularization techniques to avoid overfitting and improve generalization. Regularization helps in producing well-calibrated probability outputs by preventing extreme or overconfident predictions.\n",
      "\n",
      "6. XGBoost supports hyperparameter tuning, which allows you to optimize the model performance and adjust the balance between accuracy and calibration of probability estimates. This can be done through techniques like cross-validation and grid search.\n",
      "\n",
      "In conclusion, XGBoost Classifier is not only a powerful and accurate model but also provides reliable and well-calibrated probability outputs, making it a suitable choice when probability estimation is of interest.\n"
     ]
    }
   ],
   "source": [
    "print(chat_completion['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63b237d-4539-4587-9fe0-672d2c87337e",
   "metadata": {},
   "source": [
    "# Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa6be20-f586-466f-bfdb-c5bf0b4351e1",
   "metadata": {},
   "source": [
    "## Langchain: Text model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08eaa9d9-23c1-4528-90c3-1cdf50896be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "944e0808-3cfb-4ed4-a932-a97684eb32b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_params = {\n",
    "        \"model\": TEXT_MODEL,\n",
    "        \"openai_api_key\": OPENAI_API_KEY,\n",
    "        \"temperature\": TEMP,\n",
    "        \"max_tokens\": MAX_TOKENS\n",
    "    }\n",
    "\n",
    "llm = OpenAI(**chat_params)\n",
    "\n",
    "# llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "360d67a4-9d4b-483b-8aad-495445c91d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28b94634-b5b5-4b71-a4c4-97f2c71c887e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Yes, an XGBoost Classifier can be a good model to use if you are interested in probability outputs. XGBoost is a powerful machine learning algorithm that is used for both classification and regression problems. It is an ensemble learning algorithm that uses a combination of decision trees to make predictions. XGBoost is known for its high accuracy and speed, making it a popular choice for many machine learning tasks. Additionally, XGBoost can output probability estimates for each class, which can be useful for making predictions. Therefore, XGBoost can be a good model to use if you are interested in probability outputs.\n"
     ]
    }
   ],
   "source": [
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d61530-954a-459d-a9fa-b8cf20986a5a",
   "metadata": {},
   "source": [
    "## Langchain: Chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a48fb9d-af2a-4780-a5af-7587d8499490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25d791a5-84c7-486d-8832-03ae0e87cbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecc4af3d-b072-41eb-9f96-f9a993f86087",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_params = {\n",
    "        \"model\": CHAT_MODEL, \n",
    "        \"openai_api_key\": OPENAI_API_KEY,\n",
    "        \"temperature\": TEMP,\n",
    "        \"max_tokens\": MAX_TOKENS\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f29c224c-3eeb-47b9-80b1-4b2920d4ff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatOpenAI(**chat_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ae5d887-2e33-4e5f-afb4-8b6c33318fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply = chat_model([HumanMessage(content=prompt)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75c9db2a-abb0-4338-a6ce-8d0be4850f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, an XGBoost Classifier is a good model to use if you are interested in probability outputs. Here's a step-by-step reasoning:\n",
      "\n",
      "1. XGBoost is an optimized implementation of the gradient boosting algorithm, which is known for its high predictive accuracy. It is widely used in various machine learning competitions and real-world applications.\n",
      "\n",
      "2. XGBoost can output probability estimates for each class in a classification problem. By default, it uses a logistic regression function as the base learner, which models the probability of the positive class. This means that the output of an XGBoost Classifier can be interpreted as the probability of belonging to the positive class.\n",
      "\n",
      "3. XGBoost uses an ensemble of weak learners (decision trees) to make predictions. It combines the predictions of multiple trees to obtain the final probability estimate. Each tree contributes to the probability estimate by assigning a weight to its prediction based on its performance during training. This ensemble approach helps to improve the accuracy and reliability of the probability outputs.\n",
      "\n",
      "4. XGBoost provides a parameter called \"objective\" that allows you to specify the loss function to be optimized during training. By choosing an appropriate objective, such as \"binary:logistic\" for binary classification or \"multi:softprob\" for multi-class classification, you can directly optimize the model to output probability estimates.\n",
      "\n",
      "5. XGBoost also offers a parameter called \"eval_metric\" that allows you to specify the evaluation metric to be used during training. By choosing an appropriate evaluation metric, such as \"logloss\" for binary classification or \"mlogloss\" for multi-class classification, you can monitor the model's performance in terms of probability outputs.\n",
      "\n",
      "Overall, XGBoost's ability to output probability estimates, its ensemble approach, and its flexibility in choosing the objective and evaluation metric make it a good model to use when interested in probability outputs.\n"
     ]
    }
   ],
   "source": [
    "print(reply.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
